{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import torchvision.transforms as transforms\n",
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import collections\n",
    "from xml.etree.ElementTree import Element as ET_Element\n",
    "from xml.etree.ElementTree import parse as ET_parse\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from PIL import Image\n",
    "from google.colab.patches import cv2_imshow\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import torchvision.transforms.functional as F\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global writer\n",
    "global logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Unnormalize a tensor image with mean and standard deviation.\n",
    "    Input tensor should be of shape CxHxW.\n",
    "    mean and std are sequences of means and standard deviations per channel.\n",
    "    \"\"\"\n",
    "    # Duplicate the mean and std to match the tensor's shape\n",
    "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
    "\n",
    "    # Apply the unnormalize formula\n",
    "    unnormalized_tensor = tensor * std + mean\n",
    "\n",
    "    return unnormalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bbox_eval(image,boxes):\n",
    "    image = np.transpose(image, (1,2,0))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # image = np.zeros((540, 960, 3), dtype=np.uint8)\n",
    "    image_255 = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    # print(image_255)\n",
    "    # print(\"Shape: \" + str(image_255.shape))\n",
    "    for bbox in boxes:\n",
    "        print(\"BBox: \" + str(bbox))\n",
    "        xmin = bbox[0].item()\n",
    "        ymin = bbox[1].item()\n",
    "        xmax = bbox[2].item()\n",
    "        ymax = bbox[3].item()\n",
    "        pt1 = (int(xmin),int(ymin))\n",
    "        pt2 = (int(xmax),int(ymax))\n",
    "        print(\"Pt1: \" + str(pt1))\n",
    "        print(\"Pt2: \" + str(pt2))\n",
    "        cv2.rectangle(image_255, pt1, pt2, (0,255,0),2)\n",
    "    cv2.startWindowThread()\n",
    "    cv2_imshow(image_255)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bbox_train(image,boxes):\n",
    "    image = np.transpose(image, (1,2,0))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    image_255 = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    for bbox in boxes:\n",
    "        print(\"BBox: \" + str(bbox))\n",
    "        xmin = bbox[0]\n",
    "        ymin = bbox[1]\n",
    "        xmax = bbox[2]\n",
    "        ymax = bbox[3]\n",
    "        pt1 = (int(xmin),int(ymin))\n",
    "        pt2 = (int(xmax),int(ymax))\n",
    "        print(\"Pt1: \" + str(pt1))\n",
    "        print(\"Pt2: \" + str(pt2))\n",
    "        cv2.rectangle(image_255, pt1, pt2, (0,255,0),2)\n",
    "    cv2.startWindowThread()\n",
    "    cv2_imshow(image_255)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bbox(dataset):\n",
    "    for data in dataset:\n",
    "        # Convert the PIL.Image.Image to a NumPy array\n",
    "        image, target = data\n",
    "        print(image.shape)\n",
    "        print(image_array.shape)\n",
    "        image_array = np.array(image)\n",
    "        image_array = np.transpose(image_array, (1,2,0))\n",
    "        image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
    "        print(\"Display Target: \" + str(target))\n",
    "        for o in target[\"boxes\"]:\n",
    "            xmin = o[0].item()\n",
    "            ymin = o[1].item()\n",
    "            xmax = o[2].item()\n",
    "            ymax = o[3].item()\n",
    "            pt1 = (xmin,ymin)\n",
    "            pt2 = (xmax,ymax)\n",
    "            cv2.rectangle(image_array, pt1, pt2, (0,255,0),2)\n",
    "        cv2_imshow('Display window', image_array)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if Resize is in the transform\n",
    "def contains_resize(transform):\n",
    "    for t in transform.transforms:\n",
    "        if isinstance(t, transforms.Resize):\n",
    "            return t\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_collate(batch):\n",
    "    data = [i[0] for i in batch] \n",
    "    target = [i[1] for i in batch]\n",
    "    return data,target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashDataset(VisionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        image_set: str = \"train\",\n",
    "        transform: Optional[Callable] = None,\n",
    "        target_transform: Optional[Callable] = None,\n",
    "        transforms: Optional[Callable] = None,\n",
    "    ):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        voc_root = os.path.join(self.root,os.path.join(\"VOCdevkit\", \"VOC2012\"))\n",
    "        splits_dir = os.path.join(voc_root, \"ImageSets\", \"Main\")\n",
    "        split_f = os.path.join(splits_dir, image_set.rstrip(\"\\n\") + \".txt\")\n",
    "        with open(os.path.join(split_f)) as f:\n",
    "            file_names = [x.strip() for x in f.readlines()]\n",
    "        image_dir = os.path.join(voc_root, \"JPEGImages\")\n",
    "        self.images = [os.path.join(image_dir, x + \".jpeg\") for x in file_names]\n",
    "        target_dir = os.path.join(voc_root, \"Annotations\")\n",
    "        self.targets = [os.path.join(target_dir, x + \".xml\") for x in file_names]\n",
    "        self.imagesize = (960.0,540.0)\n",
    "        assert len(self.images) == len(self.targets)\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)\n",
    "    @property\n",
    "    def annotations(self) -> List[str]:\n",
    "        return self.targets\n",
    "    #Modified so it returns the acutal target/label format needed for training\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        img = Image.open(self.images[index]).convert(\"RGB\")\n",
    "        target = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n",
    "        objs = target[\"annotation\"][\"object\"]\n",
    "        #Temp just set it to 0?\n",
    "        labels = [1] * len(objs)\n",
    "        boxes = [[int(bbox[\"bndbox\"][\"xmin\"]), int(bbox[\"bndbox\"][\"ymin\"]), int(bbox[\"bndbox\"][\"xmax\"]), int(bbox[\"bndbox\"][\"ymax\"])] for bbox in objs]\n",
    "        target = {\"boxes\": torch.tensor(boxes), \"labels\": torch.tensor(labels)}\n",
    "        # print(\"Before Targets: \" + str(target[\"boxes\"]))\n",
    "        # if self.transforms is not None:\n",
    "        #     img, target = self.transforms(img, target)\n",
    "        if self.transform is not None:\n",
    "            img, target = self.transforms(img,target)\n",
    "        resize = contains_resize(self.transform)\n",
    "        if resize != None:\n",
    "          size_x = resize.size[0]\n",
    "          size_y = resize.size[1]\n",
    "          # Resize the bounding boxes accordingly\n",
    "          for b in target[\"boxes\"]:\n",
    "            b[0] = (b[0] / self.imagesize[0]) * size_x\n",
    "            b[1] = (b[1] / self.imagesize[1]) * size_y\n",
    "            b[2] = (b[2] / self.imagesize[0]) * size_x\n",
    "            b[3] = (b[3] / self.imagesize[1]) * size_y\n",
    "        # print(\"After Targets: \" + str(target[\"boxes\"]))\n",
    "        #   # Display to make sure the bounding boxes were scaled correctly\n",
    "        # mean = [0.485, 0.456, 0.406]\n",
    "        # std = [0.229, 0.224, 0.225]\n",
    "        # unnormalized_image = unnormalize(img,mean,std)\n",
    "        # display_bbox_train(unnormalized_image.numpy(),target[\"boxes\"])\n",
    "        return img, target\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_voc_xml(node: ET_Element) -> Dict[str, Any]:\n",
    "        voc_dict: Dict[str, Any] = {}\n",
    "        children = list(node)\n",
    "        if children:\n",
    "            def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
    "            for dc in map(TrashDataset.parse_voc_xml, children):\n",
    "                for ind, v in dc.items():\n",
    "                    def_dic[ind].append(v)\n",
    "            if node.tag == \"annotation\":\n",
    "                def_dic[\"object\"] = [def_dic[\"object\"]]\n",
    "            voc_dict = {node.tag: {ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()}}\n",
    "        if node.text:\n",
    "            text = node.text.strip()\n",
    "            if not children:\n",
    "                voc_dict[node.tag] = text\n",
    "        return voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    for images, targets in dataloader:\n",
    "        images = [image.to(device) for image in images]  # Move images to device\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move targets to device\n",
    "\n",
    "        outputs = model(images, targets)\n",
    "        print(\"Outputs: \" + str(outputs))\n",
    "        scores = outputs[0]['scores']\n",
    "        boxes = outputs[0]['boxes']\n",
    "        # Filter scores greater than 0.87\n",
    "        high_score_indices = 0\n",
    "        # Select boxes with scores above 0.87\n",
    "        high_score_boxes = boxes[high_score_indices]\n",
    "        #Only display first image?\n",
    "        unnormalized_image = unnormalize(images[0])\n",
    "        display_bbox_eval(unnormalized_image.numpy(),[high_score_boxes])\n",
    "        print(high_score_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the iamges with bounding boxes to\n",
    "def write_image(model,images,targets,threshold,global_step,title):\n",
    "  model.eval()\n",
    "  annotated_im = []\n",
    "  outputs = model(images,targets)\n",
    "  for im,out in zip(images,outputs):\n",
    "    image_pil = F.to_pil_image(unnormalize(im))  # Convert to PIL Image\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    scores = out['scores']\n",
    "    boxes = out['boxes']\n",
    "    # print(\"Scores: \" + str(scores))\n",
    "    #Always draw the top 2 boxes\n",
    "    high_score_indices = scores > threshold\n",
    "    high_score_boxes = boxes[:2]\n",
    "    high_scores = scores[:2]\n",
    "    for sc, bbox in zip(high_scores,high_score_boxes):\n",
    "        xmin = bbox[0].item()\n",
    "        ymin = bbox[1].item()\n",
    "        xmax = bbox[2].item()\n",
    "        ymax = bbox[3].item()\n",
    "        pt1 = (int(xmin),int(ymin))\n",
    "        pt2 = (int(xmax),int(ymax))\n",
    "        draw.rectangle([pt1, pt2], outline=\"green\")\n",
    "        draw.text(pt1,'{0:.2f}'.format(sc.item()),fill=(0,255,0))\n",
    "    annotated_im.append(F.to_tensor(image_pil))\n",
    "  img_grid = make_grid(annotated_im)\n",
    "  writer.add_image(title + \"-annotated_images\", img_grid, global_step=global_step)\n",
    "  model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,optimizer,num_epochs,params,weights_path):\n",
    "    log_interval = 1\n",
    "    image_log_interval = 5\n",
    "    global_step = 0\n",
    "    title = \"Loss/train/\" + str(params['lr']) + '/' + str(params['batch_size']) + '/' + str(params['num_epochs']) + '/'\n",
    "    now = datetime.now()\n",
    "    date_time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    title_with_datetime = title + date_time_str\n",
    "    for epoch in range(0,num_epochs):\n",
    "        for batch_idx,(images, targets) in enumerate(train_loader):\n",
    "            images = [image.to(device) for image in images]  # Move images to device\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move targets to device\n",
    "            outputs = model(images, targets)\n",
    "            losses = sum(loss for loss in outputs.values())\n",
    "            if batch_idx % log_interval == 0:\n",
    "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(images), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), losses.item()))\n",
    "              #Maybe only display images at a lesser frequency? Otherwise System RAM kidna takes a beating.\n",
    "              writer.add_scalar(title_with_datetime, losses.item(), global_step)\n",
    "            if batch_idx % image_log_interval == 0:\n",
    "              display_images = images[:16]\n",
    "              display_targets = targets[:16]\n",
    "              write_image(model,display_images,display_targets,0.87,global_step,title_with_datetime)\n",
    "              torch.cuda.empty_cache() #empty cache NOTE: this might be a big performance penalty I guess we'll c\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        weights_title = \"/weights\" + str(params['lr']) + '_' + str(params['batch_size']) + '_' + str(params['num_epochs']) + '_' + str(epoch)\n",
    "        torch.save(model.state_dict(),weights_path + weights_title)\n",
    "        print(\"Saved weights - Epoch: {} in {}\".format(epoch,weights_path + weights_title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters Flags\n",
    "path = 'rgb_data/'\n",
    "save = 'weights/'\n",
    "train_flag = True\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "lr = 2.5e-2\n",
    "params = {'lr': lr, 'batch_size': batch_size, 'num_epochs': num_epochs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "drive_path = '/content/drive/MyDrive/trash-model/classifier'\n",
    "data_path = os.path.join(drive_path, path)\n",
    "logs = drive_path + '/logs/tensorboard'\n",
    "writer = SummaryWriter(logs)\n",
    "%tensorboard --logdir /content/drive/MyDrive/trash-model/classifier/logs/tensorboard\n",
    "\n",
    "mobile_netv3 = models.detection.ssdlite.ssdlite320_mobilenet_v3_large(weights_backbone = \"DEFAULT\", num_classes = 2,progress = True)\n",
    "# mobile_netv3 = models.detection.ssdlite.ssdlite320_mobilenet_v3_large(pretrained = True, num_classes = 2)\n",
    "\n",
    "mobile_netv3.to(device)  # Move model to the selected device\n",
    "#Weights_backbone = pretrained=True,\n",
    "\n",
    "#Use transforms. Should just be tensors?\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness = (0.5,1.5), contrast = (0.5,1.5), saturation = (0.5,1.5), hue = (-0.1, 0.1)),\n",
    "    transforms.Resize((320,320)),\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "#TODO: SSDHEadClassificaiton()\n",
    "for param in mobile_netv3.parameters():\n",
    "    param.requires_grad = False\n",
    "for head_param in mobile_netv3.head.parameters():\n",
    "    head_param.requires_grad = True\n",
    "\n",
    "# print(\"Datapath: \" + str(data_path))\n",
    "#TODO: save weights for each epoch\n",
    "weights_string = \"trash_weights.pth\"\n",
    "weights_title = os.path.join(os.path.join(drive_path,save),weights_string)\n",
    "weights_path = os.path.join(drive_path,save)\n",
    "\n",
    "if train_flag:\n",
    "    mobile_netv3.train()\n",
    "    dataset = TrashDataset(root = data_path, image_set='train', transform = image_transforms)\n",
    "    train_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers = 2, collate_fn = my_collate)\n",
    "    optimizer = torch.optim.SGD(mobile_netv3.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "    train_model(mobile_netv3,train_dataloader, optimizer,num_epochs, params,weights_path)\n",
    "    writer.flush()\n",
    "else:\n",
    "    dataset = TrashDataset(root = data_path, image_set='val', transform = image_transforms)\n",
    "    eval_dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers = 2, collate_fn = my_collate)\n",
    "    mobile_netv3.load_state_dict(torch.load(weights_title, map_location=device))\n",
    "    mobile_netv3.eval()\n",
    "    eval_model(mobile_netv3,eval_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7500c3e1c7c786e4ba1e4b4eb7588219b4e35d5153674f92eb3a82672b534f6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
